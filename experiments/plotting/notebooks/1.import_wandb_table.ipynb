{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Generalists' (Your Algorithms') Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched entity from environment variable 'WANDB_ENTITY': jayden-teoh.\n",
      "Fetched project from environment variable 'WANDB_PROJECT': MORL-Generalization.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from rich.progress import Progress\n",
    "\n",
    "api = wandb.Api(timeout=60)\n",
    "entity = 'jayden-teoh'\n",
    "project = 'MORL-Generalization'\n",
    "TABLE_TO_EXTRACT = 'eval/discounted_front'\n",
    "ENV_NAME = \"MOLavaGridDR-v0\"\n",
    "WANDB_GROUP = \"domain_randomization\"\n",
    "if not entity:\n",
    "    raise ValueError(\"Entity not provided and environment variable 'WANDB_ENTITY' is not set.\")\n",
    "print(f\"Fetched entity from environment variable 'WANDB_ENTITY': {entity}.\")\n",
    "\n",
    "if not project:\n",
    "    raise ValueError(\"Project not provided and environment variable 'WANDB_PROJECT' is not set.\")\n",
    "print(f\"Fetched project from environment variable 'WANDB_PROJECT': {project}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs: 36\n"
     ]
    }
   ],
   "source": [
    "filters = {\"group\": WANDB_GROUP, \"tags\": {\"$in\": [ENV_NAME]}}\n",
    "try:\n",
    "    runs_sample = api.runs(path=f\"{entity}/{project}\", per_page=1, filters=filters)\n",
    "    total_runs = len(runs_sample)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Invalid entity '{entity}' or project '{project}': {str(e)}\\n\\n \\\n",
    "        Also, make sure you are properly authenticated. \\\n",
    "        You can authenticate by using 'wandb.login() or setting the environment variable 'WANDB_API_KEY'\"\n",
    "    )\n",
    "\n",
    "print(f\"Total runs: {total_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25e2d5956da47f7a372327585910974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 runs.\n"
     ]
    }
   ],
   "source": [
    "# Default CSV file name format\n",
    "date_str = datetime.now().strftime(\"%m%d%y\")\n",
    "output_file = f\"{entity}-{project}-{date_str}.csv\"\n",
    "\n",
    "all_runs_data = []\n",
    "counter = 0\n",
    "with Progress() as progress:\n",
    "    task = progress.add_task(\"[cyan]Fetching runs...\", total=total_runs)\n",
    "\n",
    "    last_created_at = None\n",
    "    while not progress.finished:\n",
    "        if last_created_at:\n",
    "            filters[\"created_at\"] = {\"$gt\": last_created_at}\n",
    "\n",
    "        runs = api.runs(path=f\"{entity}/{project}\", per_page=100, order=\"created_at\", filters=filters)\n",
    "        for run in runs:\n",
    "            # if run.state != \"finished\":\n",
    "            #     continue\n",
    "            run_data = {\n",
    "                \"name\": run.name,\n",
    "                \"state\": run.state,\n",
    "                \"path\": run.path,\n",
    "                **run.summary._json_dict,\n",
    "            }\n",
    "            all_runs_data.append(run_data)\n",
    "            progress.update(task, advance=1)\n",
    "        if len(runs) > 0:\n",
    "            last_created_at = runs[-1].created_at\n",
    "\n",
    "df = pd.DataFrame(all_runs_data)\n",
    "print(f\"Found {len(df)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [jayden-teoh, MORL-Generalization, dz8rn498]\n",
       "1     [jayden-teoh, MORL-Generalization, d8if0sgq]\n",
       "2     [jayden-teoh, MORL-Generalization, 2yx8dikb]\n",
       "3     [jayden-teoh, MORL-Generalization, ubbdh4gq]\n",
       "4     [jayden-teoh, MORL-Generalization, qmbhrxef]\n",
       "5     [jayden-teoh, MORL-Generalization, z0qzu9c8]\n",
       "6     [jayden-teoh, MORL-Generalization, pjapixg6]\n",
       "7     [jayden-teoh, MORL-Generalization, s75jijyx]\n",
       "8     [jayden-teoh, MORL-Generalization, zolc9t3l]\n",
       "9     [jayden-teoh, MORL-Generalization, tlnst047]\n",
       "10    [jayden-teoh, MORL-Generalization, w7g3kehk]\n",
       "11    [jayden-teoh, MORL-Generalization, 9ic1k3c8]\n",
       "12    [jayden-teoh, MORL-Generalization, f6d82n1e]\n",
       "13    [jayden-teoh, MORL-Generalization, zp7c7w1g]\n",
       "14    [jayden-teoh, MORL-Generalization, zo8jvxrp]\n",
       "15    [jayden-teoh, MORL-Generalization, 66e8u8b7]\n",
       "16    [jayden-teoh, MORL-Generalization, zvwgt2bw]\n",
       "17    [jayden-teoh, MORL-Generalization, ukj49wwm]\n",
       "18    [jayden-teoh, MORL-Generalization, w8476b5i]\n",
       "19    [jayden-teoh, MORL-Generalization, d1q1kg65]\n",
       "20    [jayden-teoh, MORL-Generalization, 4rue5yll]\n",
       "21    [jayden-teoh, MORL-Generalization, o60neljp]\n",
       "22    [jayden-teoh, MORL-Generalization, ecyak2uw]\n",
       "23    [jayden-teoh, MORL-Generalization, nua9umuf]\n",
       "24    [jayden-teoh, MORL-Generalization, 9an8goz7]\n",
       "25    [jayden-teoh, MORL-Generalization, qgz8urcq]\n",
       "26    [jayden-teoh, MORL-Generalization, gwy7eau1]\n",
       "27    [jayden-teoh, MORL-Generalization, r4a2r302]\n",
       "28    [jayden-teoh, MORL-Generalization, 0ovbh5vq]\n",
       "29    [jayden-teoh, MORL-Generalization, deerffu5]\n",
       "30    [jayden-teoh, MORL-Generalization, xyyzkohg]\n",
       "31    [jayden-teoh, MORL-Generalization, tqxf59al]\n",
       "32    [jayden-teoh, MORL-Generalization, cjxq1hw4]\n",
       "33    [jayden-teoh, MORL-Generalization, omyvchwo]\n",
       "34    [jayden-teoh, MORL-Generalization, mx9gyr2o]\n",
       "35    [jayden-teoh, MORL-Generalization, 99m249a8]\n",
       "Name: path, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Split the 'name' column into 'env_id', 'algorithm', 'seed', and 'time'\n",
    "df[['env_id', 'name', 'seed', 'time']] = df['name'].str.split('__', expand=True)\n",
    "\n",
    "# Drop rows with missing global_step\n",
    "df = df.dropna(subset=['global_step'])\n",
    "df['global_step'] = df['global_step'].astype(int)\n",
    "\n",
    "# Filter for only columns that start with \"eval/\", \"name\", \"path\", \"global_step\", \"env_id\", and \"seed\"\n",
    "columns_to_keep = df.filter(regex=f'^({TABLE_TO_EXTRACT}|name|path|global_step|env_id|seed)').columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Remove the \"eval/\" prefix from the column names\n",
    "df['path'] = df['path'].apply(lambda x: \"/\".join(x))\n",
    "columns_to_process = [col for col in df.columns if TABLE_TO_EXTRACT in col]\n",
    "\n",
    "output_directory = f'../data/{TABLE_TO_EXTRACT}'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for (env_id, algorithm), group in df.groupby(['env_id', 'name']):\n",
    "    algo_dir = os.path.join(output_directory, env_id, algorithm)\n",
    "    if not os.path.exists(algo_dir):\n",
    "        os.makedirs(algo_dir)\n",
    "\n",
    "    # Loop through each seed and save the corresponding data as a CSV file\n",
    "    for seed, seed_data in group.groupby('seed'):\n",
    "        seed_data = seed_data.sort_values(by='global_step')\n",
    "        seed_data.set_index('global_step', inplace=True)\n",
    "        \n",
    "        # Ensure that rows with the same global_step are merged (if necessary)\n",
    "        # This will collapse rows with the same 'global_step' by taking non-null values\n",
    "        seed_data = seed_data.groupby('global_step').first()\n",
    "\n",
    "        # Save the data for this seed\n",
    "        seed_dir = os.path.join(algo_dir, f'seed_{seed}')\n",
    "        if not os.path.exists(seed_dir):\n",
    "            os.makedirs(seed_dir)\n",
    "\n",
    "        # Restore files using wandb API\n",
    "        for col in columns_to_process:\n",
    "            artifact_path = seed_data[col].iloc[0]['path']  # Get the first (or specific) path\n",
    "            if pd.notna(artifact_path):\n",
    "                run_path = f\"{seed_data['path'].iloc[0]}\"  # Use the 'path' column to construct the run path\n",
    "                try:\n",
    "                    restored_file = wandb.restore(artifact_path, run_path=run_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to restore file for {artifact_path}: {e}\")\n",
    "\n",
    "                with open(restored_file.name, 'r') as json_file:\n",
    "                    json_data = json.load(json_file)\n",
    "                \n",
    "                columns = json_data['columns']\n",
    "                data = json_data['data']\n",
    "\n",
    "                json_df = pd.DataFrame(data, columns=columns)\n",
    "                json_csv_path = os.path.join(seed_dir, f\"{col.strip(TABLE_TO_EXTRACT)}.csv\")\n",
    "                json_df.to_csv(json_csv_path, index=False)\n",
    "\n",
    "\n",
    "print(\"Files have been successfully created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Specialists' Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched entity from environment variable 'WANDB_ENTITY': jayden-teoh.\n",
      "Fetched project from environment variable 'WANDB_PROJECT': MORL-Generalization.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from rich.progress import Progress\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from helpers.utils import ENVIRONMENTS_MAP\n",
    "\n",
    "api = wandb.Api(timeout=60)\n",
    "entity = 'jayden-teoh'\n",
    "project = 'MORL-Generalization'\n",
    "TABLE_TO_EXTRACT = 'eval/front' # same as 'eval/discounted_front' but just named wrongly\n",
    "\n",
    "ENV_NAME = \"MOHumanoidDR-v5\" # Change this to the environment you want to extract\n",
    "WANDB_GROUP = \"static_environment\"\n",
    "if not entity:\n",
    "    raise ValueError(\"Entity not provided and environment variable 'WANDB_ENTITY' is not set.\")\n",
    "print(f\"Fetched entity from environment variable 'WANDB_ENTITY': {entity}.\")\n",
    "\n",
    "if not project:\n",
    "    raise ValueError(\"Project not provided and environment variable 'WANDB_PROJECT' is not set.\")\n",
    "print(f\"Fetched project from environment variable 'WANDB_PROJECT': {project}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\"group\": WANDB_GROUP, \"tags\": {\"$in\": ENVIRONMENTS_MAP[ENV_NAME]}}\n",
    "try:\n",
    "    runs_sample = api.runs(path=f\"{entity}/{project}\", per_page=1, filters=filters)\n",
    "    total_runs = len(runs_sample)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Invalid entity '{entity}' or project '{project}': {str(e)}\\n\\n \\\n",
    "        Also, make sure you are properly authenticated. \\\n",
    "        You can authenticate by using 'wandb.login() or setting the environment variable 'WANDB_API_KEY'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bcfe94f3e64679818b0f9a61364038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 runs.\n"
     ]
    }
   ],
   "source": [
    "# Default CSV file name format\n",
    "date_str = datetime.now().strftime(\"%m%d%y\")\n",
    "output_file = f\"{entity}-{project}-{date_str}.csv\"\n",
    "\n",
    "all_runs_data = []\n",
    "counter = 0\n",
    "with Progress() as progress:\n",
    "    task = progress.add_task(\"[cyan]Fetching runs...\", total=total_runs)\n",
    "\n",
    "    last_created_at = None\n",
    "    while not progress.finished:\n",
    "        if last_created_at:\n",
    "            filters[\"created_at\"] = {\"$gt\": last_created_at}\n",
    "\n",
    "        runs = api.runs(path=f\"{entity}/{project}\", per_page=100, order=\"created_at\", filters=filters)\n",
    "        for run in runs:\n",
    "            # if run.state != \"finished\":\n",
    "            #     continue\n",
    "            run_data = {\n",
    "                \"name\": run.name,\n",
    "                \"state\": run.state,\n",
    "                \"path\": run.path,\n",
    "                **run.summary._json_dict,\n",
    "            }\n",
    "            all_runs_data.append(run_data)\n",
    "            progress.update(task, advance=1)\n",
    "        if len(runs) > 0:\n",
    "            last_created_at = runs[-1].created_at\n",
    "\n",
    "df = pd.DataFrame(all_runs_data)\n",
    "print(f\"Found {len(df)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Split the 'name' column into 'env_id', 'algorithm', 'seed', and 'time'\n",
    "df[['env_id', 'name', 'seed', 'time']] = df['name'].str.split('__', expand=True)\n",
    "\n",
    "# Drop rows with missing global_step\n",
    "df = df.dropna(subset=['global_step'])\n",
    "df['global_step'] = df['global_step'].astype(int)\n",
    "\n",
    "# Filter for only columns that start with \"eval/\", \"name\", \"path\", \"global_step\", \"env_id\" (no need seed)\n",
    "columns_to_keep = df.filter(regex=f'^({TABLE_TO_EXTRACT}|name|seed|path|global_step|env_id)').columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Remove the \"eval/\" prefix from the column names\n",
    "df['path'] = df['path'].apply(lambda x: \"/\".join(x))\n",
    "columns_to_process = [col for col in df.columns if TABLE_TO_EXTRACT in col]\n",
    "\n",
    "output_directory = f'../data/single_env/{TABLE_TO_EXTRACT}/{ENV_NAME}' # add to single_env folder\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for (env_id, algorithm, seed), group in df.groupby(['env_id', 'name', 'seed']):\n",
    "    env_dir = os.path.join(output_directory, env_id)\n",
    "    if not os.path.exists(env_dir):\n",
    "        os.makedirs(env_dir)\n",
    "\n",
    "    data = group.sort_values(by='global_step')\n",
    "    data.set_index('global_step', inplace=True)\n",
    "    data = data.groupby('global_step').first()\n",
    "\n",
    "    # Restore files using wandb API\n",
    "    for col in columns_to_process:\n",
    "        artifact_path = data[col].iloc[0]['path']  # Get the first (or specific) path\n",
    "        if pd.notna(artifact_path):\n",
    "            run_path = f\"{data['path'].iloc[0]}\"  # Use the 'path' column to construct the run path\n",
    "            try:\n",
    "                restored_file = wandb.restore(artifact_path, run_path=run_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to restore file for {artifact_path}: {e}\")\n",
    "\n",
    "            with open(restored_file.name, 'r') as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "            \n",
    "            columns = json_data['columns']\n",
    "            fetched_data = json_data['data']\n",
    "\n",
    "            json_df = pd.DataFrame(fetched_data, columns=columns)\n",
    "            json_csv_path = os.path.join(env_dir, f\"{col.strip(TABLE_TO_EXTRACT)}\", algorithm + f'-{seed}' + '.csv')\n",
    "            json_df.to_csv(json_csv_path, index=False)\n",
    "\n",
    "\n",
    "print(\"Files have been successfully created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
