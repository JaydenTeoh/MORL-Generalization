{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "ENV_NAME = \"MOLunarLanderDR-v0\" # CHANGE THIS TO THE NAME OF THE ENVIRONMENT\n",
    "REWARD_DIM = 4 # CHANGE THIS TO THE NUMBER OF OBJECTIVES IN THE ENVIRONMENT\n",
    "SEEDS = [5,26,47,76,92] # CHANGE THIS TO THE SEEDS YOU USE\n",
    "\n",
    "from helpers.utils import ENVIRONMENTS_MAP, get_algorithms\n",
    "ALGORITHMS = get_algorithms(ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Front and Calculate Normalized Hypervolume and EUM for Generalist and Specialist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../..')))\n",
    "\n",
    "def get_normalized_vec_returns(all_vec_returns, minmax_range):\n",
    "    minmax_array = np.array([minmax_range[i] for i in range(all_vec_returns.shape[-1])])\n",
    "    min_vals = minmax_array[:, 0].reshape(1, 1, -1) # reshape to (1, 1, n_objectives) for broadcasting\n",
    "    max_vals = minmax_array[:, 1].reshape(1, 1, -1)\n",
    "\n",
    "    clipped_vec_returns = np.clip(all_vec_returns, min_vals, max_vals) # broadcasted clipping\n",
    "    \n",
    "    # Normalize\n",
    "    normalized_vec_returns = (clipped_vec_returns - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    return normalized_vec_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 1.]),\n",
       " array([0.        , 0.        , 0.16343869, 0.83656131]),\n",
       " array([0.        , 0.        , 0.31034684, 0.68965316]),\n",
       " array([0.        , 0.        , 0.54624922, 0.45375078]),\n",
       " array([0.        , 0.        , 0.69973194, 0.30026806])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mo_utils.performance_indicators import hypervolume, expected_utility\n",
    "from mo_utils.weights import equally_spaced_weights\n",
    "\n",
    "NUM_WEIGHTS = 100 # CHANGE THIS TO THE NUMBER OF WEIGHTS YOU WANT TO USE, NORMALLY ITS 100, FOR MARIO ITS 32\n",
    "EVAL_WEIGHTS = equally_spaced_weights(REWARD_DIM, NUM_WEIGHTS) \n",
    "\n",
    "EVAL_WEIGHTS[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the fronts of all the Specialists for each environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found front for MOLunarLanderDefault-v0 - MORL-D(MOSACDiscrete)-SB, total row: 283\n",
      "Found front for MOLunarLanderDefault-v0 - GPI-LS, total row: 47\n",
      "Combined front for MOLunarLanderDefault-v0 has 330 rows\n",
      "objective_1, Min: -58.42785835266113, Max: 18.744779586791992\n",
      "objective_2, Min: -175.89529576146742, Max: 99.02608965285086\n",
      "objective_3, Min: -55.09010197471726, Max: 0.0\n",
      "objective_4, Min: -47.40592244267464, Max: 0.0\n",
      "Filtered front for MOLunarLanderDefault-v0 has 179 rows\n",
      "Found front for MOLunarLanderHighGravity-v0 - MORL-D(MOSACDiscrete)-SB, total row: 443\n",
      "Found front for MOLunarLanderHighGravity-v0 - GPI-LS, total row: 52\n",
      "Combined front for MOLunarLanderHighGravity-v0 has 495 rows\n",
      "objective_1, Min: -63.61854553222656, Max: -1.889656275510788\n",
      "objective_2, Min: -236.77031205231324, Max: 117.73844848107548\n",
      "objective_3, Min: -81.87934987805784, Max: 0.0\n",
      "objective_4, Min: -45.747128176689145, Max: 0.0\n",
      "Filtered front for MOLunarLanderHighGravity-v0 has 332 rows\n",
      "Found front for MOLunarLanderWindy-v0 - MORL-D(MOSACDiscrete)-SB, total row: 262\n",
      "Found front for MOLunarLanderWindy-v0 - GPI-LS, total row: 24\n",
      "Combined front for MOLunarLanderWindy-v0 has 286 rows\n",
      "objective_1, Min: -59.30862045288086, Max: 5.830689430236816\n",
      "objective_2, Min: -174.01771482739133, Max: 111.4395739948377\n",
      "objective_3, Min: -57.4398571298414, Max: 0.0\n",
      "objective_4, Min: -45.77873097658157, Max: 0.0\n",
      "Filtered front for MOLunarLanderWindy-v0 has 76 rows\n",
      "Found front for MOLunarLanderTurbulent-v0 - MORL-D(MOSACDiscrete)-SB, total row: 322\n",
      "Found front for MOLunarLanderTurbulent-v0 - GPI-LS, total row: 48\n",
      "Combined front for MOLunarLanderTurbulent-v0 has 370 rows\n",
      "objective_1, Min: -56.17124252319336, Max: 7.6819621324539185\n",
      "objective_2, Min: -411.4383911341429, Max: 86.95106956367962\n",
      "objective_3, Min: -54.4490123167634, Max: 0.0\n",
      "objective_4, Min: -60.33263351471396, Max: 0.0\n",
      "Filtered front for MOLunarLanderTurbulent-v0 has 234 rows\n",
      "Found front for MOLunarLanderHard-v0 - MORL-D(MOSACDiscrete)-SB, total row: 353\n",
      "Found front for MOLunarLanderHard-v0 - GPI-LS, total row: 56\n",
      "Combined front for MOLunarLanderHard-v0 has 409 rows\n",
      "objective_1, Min: -63.30045318603515, Max: -9.960111618041992\n",
      "objective_2, Min: -321.22631654925647, Max: 99.43525171844522\n",
      "objective_3, Min: -71.57534748017788, Max: 0.0\n",
      "objective_4, Min: -43.569594115018845, Max: 0.0\n",
      "Filtered front for MOLunarLanderHard-v0 has 231 rows\n",
      "Found front for MOLunarLanderLowMainEngine-v0 - MORL-D(MOSACDiscrete)-SB, total row: 393\n",
      "Found front for MOLunarLanderLowMainEngine-v0 - GPI-LS, total row: 46\n",
      "Combined front for MOLunarLanderLowMainEngine-v0 has 439 rows\n",
      "objective_1, Min: -56.49307632446289, Max: -13.623228454589844\n",
      "objective_2, Min: -195.9749354863889, Max: 120.05567354243249\n",
      "objective_3, Min: -74.93630290031433, Max: 0.0\n",
      "objective_4, Min: -39.371793758869174, Max: 0.0\n",
      "Filtered front for MOLunarLanderLowMainEngine-v0 has 300 rows\n",
      "Found front for MOLunarLanderStartLow-v0 - MORL-D(MOSACDiscrete)-SB, total row: 262\n",
      "Found front for MOLunarLanderStartLow-v0 - GPI-LS, total row: 49\n",
      "Combined front for MOLunarLanderStartLow-v0 has 311 rows\n",
      "objective_1, Min: -67.27579116821289, Max: 29.376918411254884\n",
      "objective_2, Min: -218.93805833617225, Max: 93.68078235373832\n",
      "objective_3, Min: -55.61540527641773, Max: 0.0\n",
      "objective_4, Min: -41.59466303884983, Max: 0.0\n",
      "Filtered front for MOLunarLanderStartLow-v0 has 216 rows\n",
      "Found front for MOLunarLanderStartRight-v0 - MORL-D(MOSACDiscrete)-SB, total row: 244\n",
      "Found front for MOLunarLanderStartRight-v0 - GPI-LS, total row: 34\n",
      "Combined front for MOLunarLanderStartRight-v0 has 278 rows\n",
      "objective_1, Min: -58.128379821777344, Max: 17.789766311645508\n",
      "objective_2, Min: -223.0651448190212, Max: 123.93594770252407\n",
      "objective_3, Min: -60.36173893511295, Max: 0.0\n",
      "objective_4, Min: -47.62996054291725, Max: 0.0\n",
      "Filtered front for MOLunarLanderStartRight-v0 has 160 rows\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from mo_utils.pareto import filter_pareto_dominated\n",
    "\n",
    "curr_envs = ENVIRONMENTS_MAP[ENV_NAME]\n",
    "SPECIALIST_FRONT = \"eval/front\" # don't change this, this is the discounted fronts but poorly named!!\n",
    "path_to_find_fronts = f\"data/single_env/{SPECIALIST_FRONT}/{ENV_NAME}\"\n",
    "\n",
    "for env in curr_envs:\n",
    "    unfiltered_combined_front_df = None\n",
    "    path_to_find_front_for_subenv = path_to_find_fronts + f\"/{env}\"\n",
    "    \n",
    "    for algo in ALGORITHMS:\n",
    "        if os.path.exists(path_to_find_front_for_subenv + f\"/{algo}.csv\"):\n",
    "            front_df = pd.read_csv(path_to_find_front_for_subenv + f\"/{algo}.csv\")\n",
    "            print(f\"Found front for {env} - {algo}, total row: {len(front_df)}\")\n",
    "            if unfiltered_combined_front_df is None:\n",
    "                unfiltered_combined_front_df = front_df\n",
    "            else:\n",
    "                unfiltered_combined_front_df = pd.concat([unfiltered_combined_front_df, front_df])\n",
    "\n",
    "    if unfiltered_combined_front_df is None:\n",
    "        warnings.warn(f\"No fronts found for {env}\")\n",
    "        continue\n",
    "    \n",
    "    unfiltered_combined_front_df = unfiltered_combined_front_df.reset_index(drop=True)\n",
    "    print(f\"Combined front for {env} has {len(unfiltered_combined_front_df)} rows\")\n",
    "\n",
    "    for column in unfiltered_combined_front_df.columns:\n",
    "        min_value = unfiltered_combined_front_df[column].min()\n",
    "        max_value = unfiltered_combined_front_df[column].max()\n",
    "        print(f\"{column}, Min: {min_value}, Max: {max_value}\")\n",
    "\n",
    "    combined_front_array = unfiltered_combined_front_df.to_numpy()\n",
    "    filtered_combined_front_array = filter_pareto_dominated(combined_front_array)\n",
    "\n",
    "    combined_front_df = pd.DataFrame(filtered_combined_front_array, columns=unfiltered_combined_front_df.columns)\n",
    "    print(f\"Filtered front for {env} has {len(combined_front_df)} rows\")\n",
    "    save_dir = f\"data/single_env/combined_fronts/{ENV_NAME}/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    combined_front_df.to_csv(f\"{save_dir}/{env}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the fronts and calculate normalized hypervolume and EUM for SPECIALIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MOLunarLanderDefault-v0': {0: [-60, 18.7],\n",
       "  1: [-175.9, 100],\n",
       "  2: [-55.1, 0],\n",
       "  3: [-47.4, 0]},\n",
       " 'MOLunarLanderHighGravity-v0': {0: [-59.3, 0],\n",
       "  1: [-174.0, 111.4],\n",
       "  2: [-57.44, 0],\n",
       "  3: [-45.7, 0]},\n",
       " 'MOLunarLanderWindy-v0': {0: [-59.3, 5.83],\n",
       "  1: [-174.0, 111.4],\n",
       "  2: [-57.44, 0],\n",
       "  3: [-45.8, 0]},\n",
       " 'MOLunarLanderTurbulent-v0': {0: [-60.2, -7.7],\n",
       "  1: [-175.4, 86.95],\n",
       "  2: [-54.4, 0],\n",
       "  3: [-60.3, 0]},\n",
       " 'MOLunarLanderHard-v0': {0: [-62.7, -9.96],\n",
       "  1: [-200.2, 99.4],\n",
       "  2: [-71.6, 0],\n",
       "  3: [-43.6, 0]},\n",
       " 'MOLunarLanderStartLow-v0': {0: [-67.3, 29.4],\n",
       "  1: [-218.9, 100],\n",
       "  2: [-67.1, 0],\n",
       "  3: [-41.6, 0]},\n",
       " 'MOLunarLanderStartRight-v0': {0: [-57.1, 17.8],\n",
       "  1: [-223.1, 123.94],\n",
       "  2: [-60.4, 0],\n",
       "  3: [-47.63, 0]},\n",
       " 'MOLunarLanderLowMainEngine-v0': {0: [-56.5, -13.6],\n",
       "  1: [-196, 120.1],\n",
       "  2: [-74.9, 0],\n",
       "  3: [-39.4, 0]}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import get_eval_params\n",
    "\n",
    "# These should exist in 'experiments/evaluation/eval_params.yaml' for current environment\n",
    "# For new environments, you can add them to the file using the minmax values from the previous step\n",
    "normalization_data = get_eval_params(ENV_NAME)['normalization']\n",
    "normalization_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import ENVIRONMENTS_MAP\n",
    "\n",
    "FRONT = \"eval/discounted_front\" # don't change this, front extracted for specialists are only the discounted ones!!\n",
    "file_path = f\"data/{FRONT}/{ENV_NAME}\"\n",
    "scores_save_path = f\"data/scores/{ENV_NAME}\"\n",
    "\n",
    "os.makedirs(f\"{scores_save_path}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_specialist_hypervolumes = []\n",
    "normalized_specialist_eums = []\n",
    "\n",
    "for env in ENVIRONMENTS_MAP[ENV_NAME]:\n",
    "    min_max_ranges = normalization_data[env]\n",
    "    best_env_front_path = f\"data/single_env/combined_fronts/{ENV_NAME}/{env}.csv\"\n",
    "    assert os.path.exists(best_env_front_path), f\"File {best_env_front_path} does not exist\"\n",
    "    \n",
    "    best_env_front = pd.read_csv(best_env_front_path)\n",
    "    data_array = best_env_front.to_numpy()\n",
    "    normalized_front = get_normalized_vec_returns(data_array, min_max_ranges)\n",
    "    normalized_specialist_hypervolumes.append(hypervolume(np.zeros(REWARD_DIM), normalized_front[0]))\n",
    "    normalized_specialist_eums.append(expected_utility(normalized_front[0], weights_set=EVAL_WEIGHTS))\n",
    "\n",
    "specialist_data = {f\"normalized_hypervolume/{env}\": [normalized_specialist_hypervolumes[i]] for i, env in enumerate(ENVIRONMENTS_MAP[ENV_NAME])}\n",
    "specialist_data.update({f\"normalized_eum/{env}\": [normalized_specialist_eums[i]] for i, env in enumerate(ENVIRONMENTS_MAP[ENV_NAME])})\n",
    "specialist_normalized_hv = pd.DataFrame(specialist_data)\n",
    "specialist_normalized_hv.to_csv(f\"{scores_save_path}/specialist.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the fronts and calculate normalized hypervolume and EUM for GENERALIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "for algo in ALGORITHMS:\n",
    "    for seed in SEEDS:\n",
    "        normalized_hypervolumes = []\n",
    "        normalized_eums = []\n",
    "        for env in ENVIRONMENTS_MAP[ENV_NAME]:\n",
    "            min_max_ranges = normalization_data[env]\n",
    "            file = f\"{file_path}/{algo}/seed_{seed}/{env}.csv\"\n",
    "            assert os.path.exists(file), f\"File {file} does not exist\"\n",
    "            data = pd.read_csv(file)\n",
    "            # Convert dataframe to numpy array of vectors\n",
    "            data_array = data.to_numpy()\n",
    "            normalized_front = get_normalized_vec_returns(data_array, min_max_ranges)\n",
    "\n",
    "            normalized_hypervolumes.append(hypervolume(np.zeros(REWARD_DIM), normalized_front[0]))\n",
    "            normalized_eums.append(expected_utility(normalized_front[0], weights_set=EVAL_WEIGHTS))\n",
    "\n",
    "        data = {f\"normalized_hypervolume/{env}\": [normalized_hypervolumes[i]] for i, env in enumerate(ENVIRONMENTS_MAP[ENV_NAME])}\n",
    "        data.update({f\"normalized_eum/{env}\": [normalized_eums[i]] for i, env in enumerate(ENVIRONMENTS_MAP[ENV_NAME])})\n",
    "        df = pd.DataFrame(data)\n",
    "        os.makedirs(f\"{scores_save_path}/{algo}/\", exist_ok=True)\n",
    "        df.to_csv(f\"{scores_save_path}/{algo}/seed_{seed}.csv\", index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate NHGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the normalized hypervolumes of the specialists\n",
    "specialist_normalized_hv_data = pd.read_csv(f\"{scores_save_path}/specialist.csv\")\n",
    "\n",
    "for algo in ALGORITHMS:\n",
    "    for seed in SEEDS:\n",
    "        nhgrs = []\n",
    "        # get the normalized hypervolumes we extracted earlier\n",
    "        file = f\"{scores_save_path}/{algo}/seed_{seed}.csv\"\n",
    "        seed_normalized_hv_data = pd.read_csv(file)\n",
    "\n",
    "        for env in ENVIRONMENTS_MAP[ENV_NAME]:\n",
    "            # Filter columns that start with \"normalized_hypervolume\"\n",
    "            col = f\"normalized_hypervolume/{env}\"\n",
    "\n",
    "            specialist_normalized_hv = specialist_normalized_hv_data[col].values[0]\n",
    "            generalist_normalized_hv = seed_normalized_hv_data[col].values[0]\n",
    "            \n",
    "            env_nhgr = generalist_normalized_hv / specialist_normalized_hv\n",
    "\n",
    "            seed_normalized_hv_data[f'NHGR/{env}'] = env_nhgr\n",
    "        \n",
    "        seed_normalized_hv_data.to_csv(f\"{scores_save_path}/{algo}/seed_{seed}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
