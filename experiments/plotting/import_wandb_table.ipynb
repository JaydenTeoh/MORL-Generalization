{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched entity from environment variable 'WANDB_ENTITY': jayden-teoh.\n",
      "Fetched project from environment variable 'WANDB_PROJECT': MORL-Baselines.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from rich.progress import Progress\n",
    "\n",
    "api = wandb.Api(timeout=60)\n",
    "entity = 'jayden-teoh'\n",
    "project = 'MORL-Baselines'\n",
    "TABLE_TO_EXTRACT = 'eval/discounted_front'\n",
    "ENV_NAME = \"MOHopperDR-v5\"\n",
    "if not entity:\n",
    "    raise ValueError(\"Entity not provided and environment variable 'WANDB_ENTITY' is not set.\")\n",
    "print(f\"Fetched entity from environment variable 'WANDB_ENTITY': {entity}.\")\n",
    "\n",
    "if not project:\n",
    "    raise ValueError(\"Project not provided and environment variable 'WANDB_PROJECT' is not set.\")\n",
    "print(f\"Fetched project from environment variable 'WANDB_PROJECT': {project}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\"group\": \"domain_randomization\"}\n",
    "try:\n",
    "    runs_sample = api.runs(path=f\"{entity}/{project}\", per_page=1)\n",
    "    total_runs = len(runs_sample)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Invalid entity '{entity}' or project '{project}': {str(e)}\\n\\nAlso, make sure you are properly authenticated. You can authenticate by using 'wandb.login() or setting the environment variable 'WANDB_API_KEY'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fb41f23ab64407b0d919c36cf47dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 355 runs to jayden-teoh-MORL-Baselines-090924.csv\n"
     ]
    }
   ],
   "source": [
    "# Default CSV file name format\n",
    "date_str = datetime.now().strftime(\"%m%d%y\")\n",
    "output_file = f\"{entity}-{project}-{date_str}.csv\"\n",
    "\n",
    "all_runs_data = []\n",
    "counter = 0\n",
    "with Progress() as progress:\n",
    "    task = progress.add_task(\"[cyan]Fetching runs...\", total=total_runs)\n",
    "\n",
    "    last_created_at = None\n",
    "    while not progress.finished:\n",
    "        filters = {\"group\": \"domain_randomization\", \"tags\": {\"$in\": [ENV_NAME]}}\n",
    "        if last_created_at:\n",
    "            filters[\"created_at\"] = {\"$gt\": last_created_at}\n",
    "\n",
    "        runs = api.runs(path=f\"{entity}/{project}\", per_page=100, order=\"created_at\", filters=filters)\n",
    "        for run in runs:\n",
    "            if run.state != \"finished\":\n",
    "                continue\n",
    "            run_data = {\n",
    "                \"name\": run.name,\n",
    "                \"state\": run.state,\n",
    "                \"path\": run.path,\n",
    "                **run.summary._json_dict,\n",
    "            }\n",
    "            all_runs_data.append(run_data)\n",
    "            progress.update(task, advance=1)\n",
    "        if len(runs) > 0:\n",
    "            last_created_at = runs[-1].created_at\n",
    "\n",
    "df = pd.DataFrame(all_runs_data)\n",
    "print(f\"Saved {len(df)} runs to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [jayden-teoh, MORL-Baselines, qpas1ji3]\n",
       "1      [jayden-teoh, MORL-Baselines, wo08mj4e]\n",
       "2      [jayden-teoh, MORL-Baselines, maqk07jm]\n",
       "3      [jayden-teoh, MORL-Baselines, 3ct7zdln]\n",
       "4      [jayden-teoh, MORL-Baselines, cgb3iwh4]\n",
       "                        ...                   \n",
       "350    [jayden-teoh, MORL-Baselines, 1qvbxxbe]\n",
       "351    [jayden-teoh, MORL-Baselines, g717o5uc]\n",
       "352    [jayden-teoh, MORL-Baselines, bseq50pl]\n",
       "353    [jayden-teoh, MORL-Baselines, 9dvz5hlu]\n",
       "354    [jayden-teoh, MORL-Baselines, iphfv84f]\n",
       "Name: path, Length: 355, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Restore files using wandb API\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns_to_process:\n\u001b[0;32m---> 55\u001b[0m     artifact_path \u001b[38;5;241m=\u001b[39m \u001b[43mseed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Get the first (or specific) path\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(artifact_path):\n\u001b[1;32m     57\u001b[0m         run_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Use the 'path' column to construct the run path\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# Split the 'name' column into 'env_id', 'algorithm', 'seed', and 'time'\n",
    "df[['env_id', 'name', 'seed', 'time']] = df['name'].str.split('__', expand=True)\n",
    "\n",
    "# Drop rows with missing global_step\n",
    "df = df.dropna(subset=['global_step'])\n",
    "df['global_step'] = df['global_step'].astype(int)\n",
    "\n",
    "# Filter for only columns that start with \"eval/\", \"name\", \"path\", \"global_step\", \"env_id\", and \"seed\"\n",
    "columns_to_keep = df.filter(regex=f'^({TABLE_TO_EXTRACT}|name|path|global_step|env_id|seed)').columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Remove the \"eval/\" prefix from the column names\n",
    "df['path'] = df['path'].apply(lambda x: \"/\".join(x))\n",
    "columns_to_process = [col for col in df.columns if TABLE_TO_EXTRACT in col]\n",
    "\n",
    "output_directory = f'data/{TABLE_TO_EXTRACT}'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for (env_id, algorithm), group in df.groupby(['env_id', 'name']):\n",
    "    algo_dir = os.path.join(output_directory, env_id, algorithm)\n",
    "    if not os.path.exists(algo_dir):\n",
    "        os.makedirs(algo_dir)\n",
    "\n",
    "    # Loop through each seed and save the corresponding data as a CSV file\n",
    "    for seed, seed_data in group.groupby('seed'):\n",
    "        seed_data = seed_data.sort_values(by='global_step')\n",
    "        seed_data.set_index('global_step', inplace=True)\n",
    "        \n",
    "        # Ensure that rows with the same global_step are merged (if necessary)\n",
    "        # This will collapse rows with the same 'global_step' by taking non-null values\n",
    "        seed_data = seed_data.groupby('global_step').first()\n",
    "\n",
    "        # Save the data for this seed\n",
    "        seed_dir = os.path.join(algo_dir, f'seed_{seed}')\n",
    "        if not os.path.exists(seed_dir):\n",
    "            os.makedirs(seed_dir)\n",
    "\n",
    "        # Restore files using wandb API\n",
    "        for col in columns_to_process:\n",
    "            artifact_path = seed_data[col].iloc[0]['path']  # Get the first (or specific) path\n",
    "            if pd.notna(artifact_path):\n",
    "                run_path = f\"{seed_data['path'].iloc[0]}\"  # Use the 'path' column to construct the run path\n",
    "                try:\n",
    "                    restored_file = wandb.restore(artifact_path, run_path=run_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to restore file for {artifact_path}: {e}\")\n",
    "\n",
    "                with open(restored_file.name, 'r') as json_file:\n",
    "                    json_data = json.load(json_file)\n",
    "                \n",
    "                columns = json_data['columns']\n",
    "                data = json_data['data']\n",
    "\n",
    "                json_df = pd.DataFrame(data, columns=columns)\n",
    "                json_csv_path = os.path.join(seed_dir, f\"{col.strip(TABLE_TO_EXTRACT)}.csv\")\n",
    "                json_df.to_csv(json_csv_path, index=False)\n",
    "\n",
    "\n",
    "print(\"Files have been successfully created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
